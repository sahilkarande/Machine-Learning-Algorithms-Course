{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries\n",
    "\n",
    "In this section, we import the necessary libraries required for data manipulation, preprocessing, and machine learning.\n",
    "\n",
    "- `numpy`: A fundamental package for numerical computations in Python.\n",
    "- `pandas`: A powerful data manipulation and analysis library.\n",
    "- `SimpleImputer`: A class from `sklearn.impute` used for handling missing data by imputing missing values.\n",
    "- `LabelEncoder`: A class from `sklearn.preprocessing` used to convert categorical labels into numerical values.\n",
    "- `OneHotEncoder`: A class from `sklearn.preprocessing` used for converting categorical variables into a format that can be provided to ML algorithms to do a better job in prediction.\n",
    "- `ColumnTransformer`: A class from `sklearn.compose` used for applying different preprocessing steps to specific columns of the dataset.\n",
    "- `train_test_split`: A function from `sklearn.model_selection` used to split the dataset into training and testing sets.\n",
    "- `StandardScaler`: A class from `sklearn.preprocessing` used for standardizing features by removing the mean and scaling to unit variance.\n",
    "\n",
    "Let's import these libraries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Dataset\n",
    "\n",
    "Here, we load the dataset from a CSV file into a DataFrame using `pandas`. This DataFrame (`df`) will allow us to inspect, manipulate, and preprocess the data.\n",
    "\n",
    "- `pd.read_csv(\"Data.csv\")`: Reads the CSV file named `Data.csv` into a DataFrame.\n",
    "- `df.head()`: Displays the first few rows of the DataFrame to give an overview of the dataset's structure and contents.\n",
    "\n",
    "Let's preview the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset preview:\n",
      "  Country  Age   Salary Purchased\n",
      "0  Mumbai   34  55000.0       Yes\n",
      "1  Nagpur   28  49000.0        No\n",
      "2    Pune   45  75000.0       Yes\n",
      "3  Nagpur   53      NaN        No\n",
      "4  Mumbai   32  58000.0       Yes\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"Data.csv\")\n",
    "print(\"Dataset preview:\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking Data Types\n",
    "\n",
    "Understanding the data types of each column is crucial for preprocessing:\n",
    "\n",
    "- `object`: Typically represents categorical data or strings.\n",
    "- `int64` and `float64`: Numeric data types, where `int64` represents integers and `float64` represents floating-point numbers.\n",
    "\n",
    "By examining the data types, we can determine appropriate preprocessing steps for each column.\n",
    "\n",
    "Let's check the data types of the columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data types of each column:\n",
      "Country       object\n",
      "Age            int64\n",
      "Salary       float64\n",
      "Purchased     object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nData types of each column:\")\n",
    "print(df.dtypes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting Independent and Dependent Variables\n",
    "\n",
    "In machine learning, we need to separate our features (independent variables) from the target variable (dependent variable):\n",
    "\n",
    "- `X`: Contains the feature columns. We select all rows and all columns except the last one.\n",
    "- `y`: Contains the target column. We select the column at index 3.\n",
    "\n",
    "These selections will be used for further preprocessing and model training.\n",
    "\n",
    "Let's extract the independent and dependent variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, :-1].values  # Independent variables (all rows, all columns except the last one)\n",
    "y = df.iloc[:, 3].values    # Dependent variable (all rows, the column at index 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Missing Values\n",
    "\n",
    "Missing values in the dataset can be imputed to ensure that our machine learning models can handle the data properly:\n",
    "\n",
    "- `SimpleImputer`: This class allows us to replace missing values with a specific strategy.\n",
    "- `strategy='mean'`: The mean of the column will replace missing values.\n",
    "\n",
    "In this case, we are applying mean imputation to columns 1 and 2 of `X`, which might contain missing values.\n",
    "\n",
    "Let's handle missing values in our dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "X[:, 1:3] = imputer.fit_transform(X[:, 1:3])  # Apply imputation to columns 1 and 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding Categorical Data\n",
    "\n",
    "Machine learning algorithms require numerical inputs, so categorical data must be converted to numerical format:\n",
    "\n",
    "- `LabelEncoder`: Converts categorical labels into numerical values (e.g., 'Yes' → 1, 'No' → 0).\n",
    "- `OneHotEncoder`: Converts categorical variables into a binary matrix (e.g., 'Mumbai' → [1, 0, 0], 'Nagpur' → [0, 1, 0]).\n",
    "\n",
    "`ColumnTransformer` applies the one-hot encoding to the specified columns, leaving the other columns unchanged.\n",
    "\n",
    "Let's encode the categorical data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelencoder_X = LabelEncoder()\n",
    "X[:, 0] = labelencoder_X.fit_transform(X[:, 0])  # Encode the categorical values in the first column\n",
    "\n",
    "ct = ColumnTransformer(\n",
    "    [('one_hot_encoder', OneHotEncoder(categories='auto'), [0])], \n",
    "    remainder='passthrough'\n",
    ")\n",
    "X = ct.fit_transform(X)  # Apply one-hot encoding to the first column\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding the Dependent Variable\n",
    "\n",
    "The target variable `y` often contains categorical labels, which need to be converted into numerical values:\n",
    "\n",
    "- `LabelEncoder`: Converts categorical labels into numerical values for easier processing by machine learning algorithms.\n",
    "\n",
    "This step transforms labels such as 'Yes' and 'No' into numerical values (e.g., 1 and 0).\n",
    "\n",
    "Let's encode the dependent variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelencoder_y = LabelEncoder()\n",
    "y = labelencoder_y.fit_transform(y)  # Encode the categorical labels in y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting the Dataset\n",
    "\n",
    "To evaluate the performance of a machine learning model, we need to split the dataset into training and testing sets:\n",
    "\n",
    "- `train_test_split`: Splits the data into training and testing subsets.\n",
    "- `test_size=0.2`: 20% of the data is used for testing, while 80% is used for training.\n",
    "- `random_state=0`: Ensures reproducibility by using a fixed random seed.\n",
    "\n",
    "Let's split the dataset into training and testing sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Scaling\n",
    "\n",
    "Feature scaling is crucial for algorithms that rely on distance calculations or are sensitive to the scale of features:\n",
    "\n",
    "- `StandardScaler`: Standardizes features by removing the mean and scaling to unit variance.\n",
    "- `fit_transform(X_train)`: Computes the mean and standard deviation from the training set and applies scaling to it.\n",
    "- `transform(X_test)`: Applies the same scaling to the test set without recalculating mean and standard deviation.\n",
    "\n",
    "Let's scale the features in the training and testing sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_X = StandardScaler()\n",
    "X_train = sc_X.fit_transform(X_train)  # Fit and transform the training set\n",
    "X_test = sc_X.transform(X_test)        # Transform the test set"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
